{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pygraphviz as pgv\n",
    "import pydot\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Scaricati/lemmas_unique_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b2cd9e78a465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../Scaricati/lemmas_unique_final.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Scaricati/lemmas_unique_final.csv'"
     ]
    }
   ],
   "source": [
    "words = pd.read_csv('../../Scaricati/lemmas_unique_final.csv', index_col = 0)\n",
    "words = words.dropna().reset_index(drop = True)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Sentiment dictionary\n",
    "\n",
    "See here for better comprehension of the sentiment dictionary\n",
    "\n",
    "http://valeriobasile.github.io/twita/sentix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Wordnet synset ID</th>\n",
       "      <th>positive score</th>\n",
       "      <th>negative score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abile</td>\n",
       "      <td>a</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>intelligente</td>\n",
       "      <td>a</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>valente</td>\n",
       "      <td>a</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>capace</td>\n",
       "      <td>a</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>incapace</td>\n",
       "      <td>a</td>\n",
       "      <td>2098</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74604</th>\n",
       "      <td>imbronciarsi</td>\n",
       "      <td>v</td>\n",
       "      <td>2771020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74605</th>\n",
       "      <td>rannuvolarsi</td>\n",
       "      <td>v</td>\n",
       "      <td>2771020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74606</th>\n",
       "      <td>rasserenarsi</td>\n",
       "      <td>v</td>\n",
       "      <td>2771169</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74607</th>\n",
       "      <td>rischiararsi</td>\n",
       "      <td>v</td>\n",
       "      <td>2771169</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74608</th>\n",
       "      <td>schiarirsi</td>\n",
       "      <td>v</td>\n",
       "      <td>2771169</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74606 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              lemma POS  Wordnet synset ID  positive score  negative score  \\\n",
       "0             abile   a               1740           0.125            0.00   \n",
       "1      intelligente   a               1740           0.125            0.00   \n",
       "2           valente   a               1740           0.125            0.00   \n",
       "3            capace   a               1740           0.125            0.00   \n",
       "4          incapace   a               2098           0.000            0.75   \n",
       "...             ...  ..                ...             ...             ...   \n",
       "74604  imbronciarsi   v            2771020           0.000            0.25   \n",
       "74605  rannuvolarsi   v            2771020           0.000            0.25   \n",
       "74606  rasserenarsi   v            2771169           0.125            0.00   \n",
       "74607  rischiararsi   v            2771169           0.125            0.00   \n",
       "74608    schiarirsi   v            2771169           0.125            0.00   \n",
       "\n",
       "       polarity  intensity  \n",
       "0           1.0      0.125  \n",
       "1           1.0      0.125  \n",
       "2           1.0      0.125  \n",
       "3           1.0      0.125  \n",
       "4          -1.0      0.750  \n",
       "...         ...        ...  \n",
       "74604      -1.0      0.250  \n",
       "74605      -1.0      0.250  \n",
       "74606       1.0      0.125  \n",
       "74607       1.0      0.125  \n",
       "74608       1.0      0.125  \n",
       "\n",
       "[74606 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentix = pd.read_csv('sentix/sentix', sep = \"\\t\", header = None, dtype = {'lemmas': object})\n",
    "sentix.columns = ['lemma', 'POS', 'Wordnet synset ID' ,'positive score', 'negative score', 'polarity','intensity']\n",
    "sentix = sentix.dropna()\n",
    "sentix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(pos, neg):\n",
    "    \n",
    "    if (pos == 0.)and(neg == 0.):\n",
    "        return 0.\n",
    "    \n",
    "    elif pos != 0.:\n",
    "        theta = np.arctan(neg/pos)\n",
    "    \n",
    "    elif pos == 0: \n",
    "        theta = np.arctan(np.Inf)\n",
    "    \n",
    "    return 1.-4.*theta/np.pi\n",
    "\n",
    "def get_exact_match(word, sentix):\n",
    "    subset = sentix[sentix.loc[:,'lemma'].str.fullmatch(word) == True]\n",
    "    \n",
    "    if (len(subset) == 0):\n",
    "        return subset.loc[:,['lemma','positive score','negative score','polarity','intensity']]\n",
    "    \n",
    "    if (len(subset) == 1):\n",
    "        return subset.loc[:,['lemma','positive score','negative score','polarity','intensity']]\n",
    "        \n",
    "    elif (len(subset) > 1):\n",
    "        pos_mean  = subset.loc[:,'positive score'].mean()\n",
    "        neg_mean  = subset.loc[:,'negative score'].mean()\n",
    "        polarity  = get_polarity(pos_mean,neg_mean)\n",
    "        intensity = np.sqrt(pos_mean**2 + neg_mean**2)\n",
    "        \n",
    "        dictionary = {'lemma': [word], 'positive score' : [pos_mean],'negative score' : [neg_mean],\n",
    "                             'polarity' : [polarity],'intensity' : [intensity]}\n",
    "        \n",
    "        return pd.DataFrame.from_dict(dictionary)\n",
    "    \n",
    "def get_match(word, sentix):\n",
    "    subset = sentix[sentix.loc[:,'lemma'].str.match(word) == True]\n",
    "    \n",
    "    if (len(subset) == 0):\n",
    "        return subset.loc[:,['lemma','positive score','negative score','polarity','intensity']]\n",
    "    \n",
    "    if (len(subset) == 1):\n",
    "        return subset.loc[:,['lemma','positive score','negative score','polarity','intensity']]\n",
    "        \n",
    "    elif (len(subset) > 1):\n",
    "        pos_mean  = subset.loc[:,'positive score'].mean()\n",
    "        neg_mean  = subset.loc[:,'negative score'].mean()\n",
    "        polarity  = get_polarity(pos_mean,neg_mean)\n",
    "        intensity = np.sqrt(pos_mean**2 + neg_mean**2)\n",
    "        \n",
    "        dictionary = {'lemma': [word], 'positive score' : [pos_mean],'negative score' : [neg_mean],\n",
    "                             'polarity' : [polarity],'intensity' : [intensity]}\n",
    "        \n",
    "        return pd.DataFrame.from_dict(dictionary)\n",
    "    \n",
    "def get_contains(word, sentix):\n",
    "    subset = sentix[sentix.loc[:,'lemma'].str.contains(word) == True]\n",
    "    \n",
    "    if (len(subset) == 0):\n",
    "        return subset.loc[:,['lemma','positive score','negative score','polarity','intensity']]\n",
    "    \n",
    "    if (len(subset) == 1):\n",
    "        return subset.loc[:,['lemma','positive score','negative score','polarity','intensity']]\n",
    "        \n",
    "    elif (len(subset) > 1):\n",
    "        pos_mean  = subset.loc[:,'positive score'].mean()\n",
    "        neg_mean  = subset.loc[:,'negative score'].mean()\n",
    "        polarity  = get_polarity(pos_mean,neg_mean)\n",
    "        intensity = np.sqrt(pos_mean**2 + neg_mean**2)\n",
    "        \n",
    "        dictionary = {'lemma': [word], 'positive score' : [pos_mean],'negative score' : [neg_mean],\n",
    "                             'polarity' : [polarity],'intensity' : [intensity]}\n",
    "        \n",
    "        return pd.DataFrame.from_dict(dictionary)\n",
    "    \n",
    "def get_similar(word, sentix):\n",
    "    similar_words = difflib.get_close_matches(word, sentix.lemma, n = 8, cutoff = 0.8)\n",
    "    subset = sentix[sentix.loc[:,'lemma'].isin(similar_words) == True]\n",
    "    \n",
    "    if (len(subset) == 0):\n",
    "        return subset.loc[:,['lemma','positive score','negative score','polarity','intensity']]\n",
    "    \n",
    "    if (len(subset) == 1):\n",
    "        return subset.loc[:,['lemma','positive score','negative score','polarity','intensity']]\n",
    "        \n",
    "    elif (len(subset) > 1):\n",
    "        pos_mean  = subset.loc[:,'positive score'].mean()\n",
    "        neg_mean  = subset.loc[:,'negative score'].mean()\n",
    "        polarity  = get_polarity(pos_mean ,neg_mean)\n",
    "        intensity = np.sqrt(pos_mean**2 + neg_mean**2)\n",
    "        \n",
    "        dictionary = {'lemma': [word], 'positive score' : [pos_mean],'negative score' : [neg_mean],\n",
    "                             'polarity' : [polarity],'intensity' : [intensity]}\n",
    "        \n",
    "        return pd.DataFrame.from_dict(dictionary)\n",
    "\n",
    "#instead of returning a row of a dictionary, return a list of its values\n",
    "def denest_row(row):\n",
    "    \n",
    "    return [row.loc[:,'lemma'].values[0], \n",
    "            row.loc[:,'positive score'].values[0], \n",
    "            row.loc[:,'negative score'].values[0], \n",
    "            row.loc[:,'polarity'].values[0],\n",
    "            row.loc[:,'intensity'].values[0]]\n",
    "\n",
    "def get_rows_data(word, sentix, use_similarity = False):\n",
    "    \n",
    "    row = get_exact_match(word, sentix)\n",
    "    \n",
    "    #if there are no exact matches, try others\n",
    "    if (len(row) == 0):\n",
    "#         print('no exact match')\n",
    "        row = get_match(word, sentix)\n",
    "        \n",
    "        #if there are no matches, try the others\n",
    "        if (len(row) == 0):\n",
    "#             print('no match')\n",
    "            row = get_contains(word, sentix)\n",
    "            \n",
    "            #if word is not even contained and we want similarities then try it\n",
    "            if ((len(row) == 0) and (use_similarity)):\n",
    "#                 print('no contains')\n",
    "                row = get_similar(word, sentix)\n",
    "            \n",
    "                #if still no matches, give up and return an empty dataframe\n",
    "                if (len(row) == 0):\n",
    "#                     print('no similar')\n",
    "                    dictionary = {'lemma': [word], 'positive score' : [0.],'negative score' : [0.],\n",
    "                                  'polarity' : [0.],'intensity' : [0.]}\n",
    "                    row =  pd.DataFrame.from_dict(dictionary)\n",
    "            \n",
    "            #if we do not want similarities then return the empty dataframe        \n",
    "            elif (len(row) == 0):\n",
    "#                 print('no contains')\n",
    "                dictionary = {'lemma': [word], 'positive score' : [0.],'negative score' : [0.],\n",
    "                                  'polarity' : [0.],'intensity' : [0.]}\n",
    "                row = pd.DataFrame.from_dict(dictionary)\n",
    "        \n",
    "    #get back the original word    \n",
    "    row.lemma = word\n",
    "    \n",
    "    #compute the right polarity\n",
    "    row.polarity = get_polarity(row.loc[0,'positive score'] , row.loc[0,'negative score'] )\n",
    "    \n",
    "    #denest it\n",
    "    return denest_row(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next call to the function get_rows_data(word, sentix, use_similarity = False) above does the following:\n",
    "\n",
    "+ **Output**: Returns a row of the kind (lemma, positive score, negative score, polarity, intensity)\n",
    "\n",
    "### **What it does**: it looks for a matching word in the sentix dataframe. \n",
    "* If there is one *exactly* matching, returns that row\n",
    "* if there are many, then compute the mean between the two positive and negative scores among the different rows, polarity is the sign of the difference (positive_score - negative_score) and intensity is the $L_2$ norm (i.e. sqrt(pos^2 + neg^2))\n",
    "* If there is **NONE** exactly matching, then look for any string that *matches* a word in the sentix dictionary. Same reasoning as before. If there is only one matching then returns that row, otherwise do some algebra.\n",
    "\n",
    "* If still there are none matching, then look for any string that *contains* (indeed to *contain* is a less strict relation that *matching* that in turn is a less strict relation than *exactly match*)\n",
    "\n",
    "* If still there are none contained, if a flag is set to true, then take the most similar words in the lemmas list and use them to create sentiment. \n",
    "\n",
    "* If even this way there are none, or the previous flag was set to false, return a row of the kind      (word, 0.  ,  0.  , 0.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['lemma', 'positive_score', 'negative_score', 'polarity', 'intensity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word # 1 / 40019\n",
      "word # 51 / 40019\n",
      "word # 101 / 40019\n",
      "word # 151 / 40019\n",
      "word # 201 / 40019\n",
      "word # 251 / 40019\n",
      "word # 301 / 40019\n",
      "word # 351 / 40019\n",
      "word # 401 / 40019\n",
      "word # 451 / 40019\n",
      "word # 501 / 40019\n",
      "word # 551 / 40019\n",
      "word # 601 / 40019\n",
      "word # 651 / 40019\n",
      "word # 701 / 40019\n",
      "word # 751 / 40019\n",
      "word # 801 / 40019\n",
      "word # 851 / 40019\n",
      "word # 901 / 40019\n",
      "word # 951 / 40019\n",
      "word # 1001 / 40019\n",
      "word # 1051 / 40019\n",
      "word # 1101 / 40019\n",
      "word # 1151 / 40019\n",
      "word # 1201 / 40019\n",
      "word # 1251 / 40019\n",
      "word # 1301 / 40019\n",
      "word # 1351 / 40019\n",
      "word # 1401 / 40019\n",
      "word # 1451 / 40019\n",
      "word # 1501 / 40019\n",
      "word # 1551 / 40019\n",
      "word # 1601 / 40019\n",
      "word # 1651 / 40019\n",
      "word # 1701 / 40019\n",
      "word # 1751 / 40019\n",
      "word # 1801 / 40019\n",
      "word # 1851 / 40019\n",
      "word # 1901 / 40019\n",
      "word # 1951 / 40019\n",
      "word # 2001 / 40019\n",
      "word # 2051 / 40019\n",
      "word # 2101 / 40019\n",
      "word # 2151 / 40019\n",
      "word # 2201 / 40019\n",
      "word # 2251 / 40019\n",
      "word # 2301 / 40019\n",
      "word # 2351 / 40019\n",
      "word # 2401 / 40019\n",
      "word # 2451 / 40019\n",
      "word # 2501 / 40019\n",
      "word # 2551 / 40019\n",
      "word # 2601 / 40019\n",
      "word # 2651 / 40019\n",
      "word # 2701 / 40019\n",
      "word # 2751 / 40019\n",
      "word # 2801 / 40019\n",
      "word # 2851 / 40019\n",
      "word # 2901 / 40019\n",
      "word # 2951 / 40019\n",
      "word # 3001 / 40019\n",
      "word # 3051 / 40019\n",
      "word # 3101 / 40019\n",
      "word # 3151 / 40019\n",
      "word # 3201 / 40019\n",
      "word # 3251 / 40019\n",
      "word # 3301 / 40019\n",
      "word # 3351 / 40019\n",
      "word # 3401 / 40019\n",
      "word # 3451 / 40019\n",
      "word # 3501 / 40019\n",
      "word # 3551 / 40019\n",
      "word # 3601 / 40019\n",
      "word # 3651 / 40019\n",
      "word # 3701 / 40019\n",
      "word # 3751 / 40019\n",
      "word # 3801 / 40019\n",
      "word # 3851 / 40019\n",
      "word # 3901 / 40019\n",
      "word # 3951 / 40019\n",
      "word # 4001 / 40019\n",
      "word # 4051 / 40019\n",
      "word # 4101 / 40019\n",
      "word # 4151 / 40019\n",
      "word # 4201 / 40019\n",
      "word # 4251 / 40019\n",
      "word # 4301 / 40019\n",
      "word # 4351 / 40019\n",
      "word # 4401 / 40019\n",
      "word # 4451 / 40019\n",
      "word # 4501 / 40019\n",
      "word # 4551 / 40019\n",
      "word # 4601 / 40019\n",
      "word # 4651 / 40019\n",
      "word # 4701 / 40019\n",
      "word # 4751 / 40019\n",
      "word # 4801 / 40019\n",
      "word # 4851 / 40019\n",
      "word # 4901 / 40019\n",
      "word # 4951 / 40019\n",
      "word # 5001 / 40019\n",
      "word # 5051 / 40019\n",
      "word # 5101 / 40019\n",
      "word # 5151 / 40019\n",
      "word # 5201 / 40019\n",
      "word # 5251 / 40019\n",
      "word # 5301 / 40019\n",
      "word # 5351 / 40019\n",
      "word # 5401 / 40019\n",
      "word # 5451 / 40019\n",
      "word # 5501 / 40019\n",
      "word # 5551 / 40019\n",
      "word # 5601 / 40019\n",
      "word # 5651 / 40019\n",
      "word # 5701 / 40019\n",
      "word # 5751 / 40019\n",
      "word # 5801 / 40019\n",
      "word # 5851 / 40019\n",
      "word # 5901 / 40019\n",
      "word # 5951 / 40019\n",
      "word # 6001 / 40019\n",
      "word # 6051 / 40019\n",
      "word # 6101 / 40019\n",
      "word # 6151 / 40019\n",
      "word # 6201 / 40019\n",
      "word # 6251 / 40019\n",
      "word # 6301 / 40019\n",
      "word # 6351 / 40019\n",
      "word # 6401 / 40019\n",
      "word # 6451 / 40019\n",
      "word # 6501 / 40019\n",
      "word # 6551 / 40019\n",
      "word # 6601 / 40019\n",
      "word # 6651 / 40019\n",
      "word # 6701 / 40019\n",
      "word # 6751 / 40019\n",
      "word # 6801 / 40019\n",
      "word # 6851 / 40019\n",
      "word # 6901 / 40019\n",
      "word # 6951 / 40019\n",
      "word # 7001 / 40019\n",
      "word # 7051 / 40019\n",
      "word # 7101 / 40019\n",
      "word # 7151 / 40019\n",
      "word # 7201 / 40019\n",
      "word # 7251 / 40019\n",
      "word # 7301 / 40019\n",
      "word # 7351 / 40019\n",
      "word # 7401 / 40019\n",
      "word # 7451 / 40019\n",
      "word # 7501 / 40019\n",
      "word # 7551 / 40019\n",
      "word # 7601 / 40019\n",
      "word # 7651 / 40019\n",
      "word # 7701 / 40019\n",
      "word # 7751 / 40019\n",
      "word # 7801 / 40019\n",
      "word # 7851 / 40019\n",
      "word # 7901 / 40019\n",
      "word # 7951 / 40019\n",
      "word # 8001 / 40019\n",
      "word # 8051 / 40019\n",
      "word # 8101 / 40019\n",
      "word # 8151 / 40019\n",
      "word # 8201 / 40019\n",
      "word # 8251 / 40019\n",
      "word # 8301 / 40019\n",
      "word # 8351 / 40019\n",
      "word # 8401 / 40019\n",
      "word # 8451 / 40019\n",
      "word # 8501 / 40019\n",
      "word # 8551 / 40019\n",
      "word # 8601 / 40019\n",
      "word # 8651 / 40019\n",
      "word # 8701 / 40019\n",
      "word # 8751 / 40019\n",
      "word # 8801 / 40019\n",
      "word # 8851 / 40019\n",
      "word # 8901 / 40019\n",
      "word # 8951 / 40019\n",
      "word # 9001 / 40019\n",
      "word # 9051 / 40019\n",
      "word # 9101 / 40019\n",
      "word # 9151 / 40019\n",
      "word # 9201 / 40019\n",
      "word # 9251 / 40019\n",
      "word # 9301 / 40019\n",
      "word # 9351 / 40019\n",
      "word # 9401 / 40019\n",
      "word # 9451 / 40019\n",
      "word # 9501 / 40019\n",
      "word # 9551 / 40019\n",
      "word # 9601 / 40019\n",
      "word # 9651 / 40019\n",
      "word # 9701 / 40019\n",
      "word # 9751 / 40019\n",
      "word # 9801 / 40019\n",
      "word # 9851 / 40019\n",
      "word # 9901 / 40019\n",
      "word # 9951 / 40019\n",
      "word # 10001 / 40019\n",
      "word # 10051 / 40019\n",
      "word # 10101 / 40019\n",
      "word # 10151 / 40019\n",
      "word # 10201 / 40019\n",
      "word # 10251 / 40019\n",
      "word # 10301 / 40019\n",
      "word # 10351 / 40019\n",
      "word # 10401 / 40019\n",
      "word # 10451 / 40019\n",
      "word # 10501 / 40019\n",
      "word # 10551 / 40019\n",
      "word # 10601 / 40019\n",
      "word # 10651 / 40019\n",
      "word # 10701 / 40019\n",
      "word # 10751 / 40019\n",
      "word # 10801 / 40019\n",
      "word # 10851 / 40019\n",
      "word # 10901 / 40019\n",
      "word # 10951 / 40019\n",
      "word # 11001 / 40019\n",
      "word # 11051 / 40019\n",
      "word # 11101 / 40019\n",
      "word # 11151 / 40019\n",
      "word # 11201 / 40019\n",
      "word # 11251 / 40019\n",
      "word # 11301 / 40019\n",
      "word # 11351 / 40019\n",
      "word # 11401 / 40019\n",
      "word # 11451 / 40019\n",
      "word # 11501 / 40019\n",
      "word # 11551 / 40019\n",
      "word # 11601 / 40019\n",
      "word # 11651 / 40019\n",
      "word # 11701 / 40019\n",
      "word # 11751 / 40019\n",
      "word # 11801 / 40019\n",
      "word # 11851 / 40019\n",
      "word # 11901 / 40019\n",
      "word # 11951 / 40019\n",
      "word # 12001 / 40019\n",
      "word # 12051 / 40019\n",
      "word # 12101 / 40019\n",
      "word # 12151 / 40019\n",
      "word # 12201 / 40019\n",
      "word # 12251 / 40019\n",
      "word # 12301 / 40019\n",
      "word # 12351 / 40019\n",
      "word # 12401 / 40019\n",
      "word # 12451 / 40019\n",
      "word # 12501 / 40019\n",
      "word # 12551 / 40019\n",
      "word # 12601 / 40019\n",
      "word # 12651 / 40019\n",
      "word # 12701 / 40019\n",
      "word # 12751 / 40019\n",
      "word # 12801 / 40019\n",
      "word # 12851 / 40019\n",
      "word # 12901 / 40019\n",
      "word # 12951 / 40019\n",
      "word # 13001 / 40019\n",
      "word # 13051 / 40019\n",
      "word # 13101 / 40019\n",
      "word # 13151 / 40019\n",
      "word # 13201 / 40019\n",
      "word # 13251 / 40019\n",
      "word # 13301 / 40019\n",
      "word # 13351 / 40019\n",
      "word # 13401 / 40019\n",
      "word # 13451 / 40019\n",
      "word # 13501 / 40019\n",
      "word # 13551 / 40019\n",
      "word # 13601 / 40019\n",
      "word # 13651 / 40019\n",
      "word # 13701 / 40019\n",
      "word # 13751 / 40019\n",
      "word # 13801 / 40019\n",
      "word # 13851 / 40019\n",
      "word # 13901 / 40019\n",
      "word # 13951 / 40019\n",
      "word # 14001 / 40019\n",
      "word # 14051 / 40019\n",
      "word # 14101 / 40019\n",
      "word # 14151 / 40019\n",
      "word # 14201 / 40019\n",
      "word # 14251 / 40019\n",
      "word # 14301 / 40019\n",
      "word # 14351 / 40019\n",
      "word # 14401 / 40019\n",
      "word # 14451 / 40019\n",
      "word # 14501 / 40019\n",
      "word # 14551 / 40019\n",
      "word # 14601 / 40019\n",
      "word # 14651 / 40019\n",
      "word # 14701 / 40019\n",
      "word # 14751 / 40019\n",
      "word # 14801 / 40019\n",
      "word # 14851 / 40019\n",
      "word # 14901 / 40019\n",
      "word # 14951 / 40019\n",
      "word # 15001 / 40019\n",
      "word # 15051 / 40019\n",
      "word # 15101 / 40019\n",
      "word # 15151 / 40019\n",
      "word # 15201 / 40019\n",
      "word # 15251 / 40019\n",
      "word # 15301 / 40019\n",
      "word # 15351 / 40019\n",
      "word # 15401 / 40019\n",
      "word # 15451 / 40019\n",
      "word # 15501 / 40019\n",
      "word # 15551 / 40019\n",
      "word # 15601 / 40019\n",
      "word # 15651 / 40019\n",
      "word # 15701 / 40019\n",
      "word # 15751 / 40019\n",
      "word # 15801 / 40019\n",
      "word # 15851 / 40019\n",
      "word # 15901 / 40019\n",
      "word # 15951 / 40019\n",
      "word # 16001 / 40019\n",
      "word # 16051 / 40019\n",
      "word # 16101 / 40019\n",
      "word # 16151 / 40019\n",
      "word # 16201 / 40019\n",
      "word # 16251 / 40019\n",
      "word # 16301 / 40019\n",
      "word # 16351 / 40019\n",
      "word # 16401 / 40019\n",
      "word # 16451 / 40019\n",
      "word # 16501 / 40019\n",
      "word # 16551 / 40019\n",
      "word # 16601 / 40019\n",
      "word # 16651 / 40019\n",
      "word # 16701 / 40019\n",
      "word # 16751 / 40019\n",
      "word # 16801 / 40019\n",
      "word # 16851 / 40019\n",
      "word # 16901 / 40019\n",
      "word # 16951 / 40019\n",
      "word # 17001 / 40019\n",
      "word # 17051 / 40019\n",
      "word # 17101 / 40019\n",
      "word # 17151 / 40019\n",
      "word # 17201 / 40019\n",
      "word # 17251 / 40019\n",
      "word # 17301 / 40019\n",
      "word # 17351 / 40019\n",
      "word # 17401 / 40019\n",
      "word # 17451 / 40019\n",
      "word # 17501 / 40019\n",
      "word # 17551 / 40019\n",
      "word # 17601 / 40019\n",
      "word # 17651 / 40019\n",
      "word # 17701 / 40019\n",
      "word # 17751 / 40019\n",
      "word # 17801 / 40019\n",
      "word # 17851 / 40019\n",
      "word # 17901 / 40019\n",
      "word # 17951 / 40019\n",
      "word # 18001 / 40019\n",
      "word # 18051 / 40019\n",
      "word # 18101 / 40019\n",
      "word # 18151 / 40019\n",
      "word # 18201 / 40019\n",
      "word # 18251 / 40019\n",
      "word # 18301 / 40019\n",
      "word # 18351 / 40019\n",
      "word # 18401 / 40019\n",
      "word # 18451 / 40019\n",
      "word # 18501 / 40019\n",
      "word # 18551 / 40019\n",
      "word # 18601 / 40019\n",
      "word # 18651 / 40019\n",
      "word # 18701 / 40019\n",
      "word # 18751 / 40019\n",
      "word # 18801 / 40019\n",
      "word # 18851 / 40019\n",
      "word # 18901 / 40019\n",
      "word # 18951 / 40019\n",
      "word # 19001 / 40019\n",
      "word # 19051 / 40019\n",
      "word # 19101 / 40019\n",
      "word # 19151 / 40019\n",
      "word # 19201 / 40019\n",
      "word # 19251 / 40019\n",
      "word # 19301 / 40019\n",
      "word # 19351 / 40019\n",
      "word # 19401 / 40019\n",
      "word # 19451 / 40019\n",
      "word # 19501 / 40019\n",
      "word # 19551 / 40019\n",
      "word # 19601 / 40019\n",
      "word # 19651 / 40019\n",
      "word # 19701 / 40019\n",
      "word # 19751 / 40019\n",
      "word # 19801 / 40019\n",
      "word # 19851 / 40019\n",
      "word # 19901 / 40019\n",
      "word # 19951 / 40019\n",
      "word # 20001 / 40019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word # 20051 / 40019\n",
      "word # 20101 / 40019\n",
      "word # 20151 / 40019\n",
      "word # 20201 / 40019\n",
      "word # 20251 / 40019\n",
      "word # 20301 / 40019\n",
      "word # 20351 / 40019\n",
      "word # 20401 / 40019\n",
      "word # 20451 / 40019\n",
      "word # 20501 / 40019\n",
      "word # 20551 / 40019\n",
      "word # 20601 / 40019\n",
      "word # 20651 / 40019\n",
      "word # 20701 / 40019\n",
      "word # 20751 / 40019\n",
      "word # 20801 / 40019\n",
      "word # 20851 / 40019\n",
      "word # 20901 / 40019\n",
      "word # 20951 / 40019\n",
      "word # 21001 / 40019\n",
      "word # 21051 / 40019\n",
      "word # 21101 / 40019\n",
      "word # 21151 / 40019\n",
      "word # 21201 / 40019\n",
      "word # 21251 / 40019\n",
      "word # 21301 / 40019\n",
      "word # 21351 / 40019\n",
      "word # 21401 / 40019\n",
      "word # 21451 / 40019\n",
      "word # 21501 / 40019\n",
      "word # 21551 / 40019\n",
      "word # 21601 / 40019\n",
      "word # 21651 / 40019\n",
      "word # 21701 / 40019\n",
      "word # 21751 / 40019\n",
      "word # 21801 / 40019\n",
      "word # 21851 / 40019\n",
      "word # 21901 / 40019\n",
      "word # 21951 / 40019\n",
      "word # 22001 / 40019\n",
      "word # 22051 / 40019\n",
      "word # 22101 / 40019\n",
      "word # 22151 / 40019\n",
      "word # 22201 / 40019\n",
      "word # 22251 / 40019\n",
      "word # 22301 / 40019\n",
      "word # 22351 / 40019\n",
      "word # 22401 / 40019\n",
      "word # 22451 / 40019\n",
      "word # 22501 / 40019\n",
      "word # 22551 / 40019\n",
      "word # 22601 / 40019\n",
      "word # 22651 / 40019\n",
      "word # 22701 / 40019\n",
      "word # 22751 / 40019\n",
      "word # 22801 / 40019\n",
      "word # 22851 / 40019\n",
      "word # 22901 / 40019\n",
      "word # 22951 / 40019\n",
      "word # 23001 / 40019\n",
      "word # 23051 / 40019\n",
      "word # 23101 / 40019\n",
      "word # 23151 / 40019\n",
      "word # 23201 / 40019\n",
      "word # 23251 / 40019\n",
      "word # 23301 / 40019\n",
      "word # 23351 / 40019\n",
      "word # 23401 / 40019\n",
      "word # 23451 / 40019\n",
      "word # 23501 / 40019\n",
      "word # 23551 / 40019\n",
      "word # 23601 / 40019\n",
      "word # 23651 / 40019\n",
      "word # 23701 / 40019\n",
      "word # 23751 / 40019\n",
      "word # 23801 / 40019\n",
      "word # 23851 / 40019\n",
      "word # 23901 / 40019\n",
      "word # 23951 / 40019\n",
      "word # 24001 / 40019\n",
      "word # 24051 / 40019\n",
      "word # 24101 / 40019\n",
      "word # 24151 / 40019\n",
      "word # 24201 / 40019\n",
      "word # 24251 / 40019\n",
      "word # 24301 / 40019\n",
      "word # 24351 / 40019\n",
      "word # 24401 / 40019\n",
      "word # 24451 / 40019\n",
      "word # 24501 / 40019\n",
      "word # 24551 / 40019\n",
      "word # 24601 / 40019\n",
      "word # 24651 / 40019\n",
      "word # 24701 / 40019\n",
      "word # 24751 / 40019\n",
      "word # 24801 / 40019\n",
      "word # 24851 / 40019\n",
      "word # 24901 / 40019\n",
      "word # 24951 / 40019\n",
      "word # 25001 / 40019\n",
      "word # 25051 / 40019\n",
      "word # 25101 / 40019\n",
      "word # 25151 / 40019\n",
      "word # 25201 / 40019\n",
      "word # 25251 / 40019\n",
      "word # 25301 / 40019\n",
      "word # 25351 / 40019\n",
      "word # 25401 / 40019\n",
      "word # 25451 / 40019\n",
      "word # 25501 / 40019\n",
      "word # 25551 / 40019\n",
      "word # 25601 / 40019\n",
      "word # 25651 / 40019\n",
      "word # 25701 / 40019\n",
      "word # 25751 / 40019\n",
      "word # 25801 / 40019\n",
      "word # 25851 / 40019\n",
      "word # 25901 / 40019\n",
      "word # 25951 / 40019\n",
      "word # 26001 / 40019\n",
      "word # 26051 / 40019\n",
      "word # 26101 / 40019\n",
      "word # 26151 / 40019\n",
      "word # 26201 / 40019\n",
      "word # 26251 / 40019\n",
      "word # 26301 / 40019\n",
      "word # 26351 / 40019\n",
      "word # 26401 / 40019\n",
      "word # 26451 / 40019\n",
      "word # 26501 / 40019\n",
      "word # 26551 / 40019\n",
      "word # 26601 / 40019\n",
      "word # 26651 / 40019\n",
      "word # 26701 / 40019\n",
      "word # 26751 / 40019\n",
      "word # 26801 / 40019\n",
      "word # 26851 / 40019\n",
      "word # 26901 / 40019\n",
      "word # 26951 / 40019\n",
      "word # 27001 / 40019\n",
      "word # 27051 / 40019\n",
      "word # 27101 / 40019\n",
      "word # 27151 / 40019\n",
      "word # 27201 / 40019\n",
      "word # 27251 / 40019\n",
      "word # 27301 / 40019\n",
      "word # 27351 / 40019\n",
      "word # 27401 / 40019\n",
      "word # 27451 / 40019\n",
      "word # 27501 / 40019\n",
      "word # 27551 / 40019\n",
      "word # 27601 / 40019\n",
      "word # 27651 / 40019\n",
      "word # 27701 / 40019\n",
      "word # 27751 / 40019\n",
      "word # 27801 / 40019\n",
      "word # 27851 / 40019\n",
      "word # 27901 / 40019\n",
      "word # 27951 / 40019\n",
      "word # 28001 / 40019\n",
      "word # 28051 / 40019\n",
      "word # 28101 / 40019\n",
      "word # 28151 / 40019\n",
      "word # 28201 / 40019\n",
      "word # 28251 / 40019\n",
      "word # 28301 / 40019\n",
      "word # 28351 / 40019\n",
      "word # 28401 / 40019\n",
      "word # 28451 / 40019\n",
      "word # 28501 / 40019\n",
      "word # 28551 / 40019\n",
      "word # 28601 / 40019\n",
      "word # 28651 / 40019\n",
      "word # 28701 / 40019\n",
      "word # 28751 / 40019\n",
      "word # 28801 / 40019\n",
      "word # 28851 / 40019\n",
      "word # 28901 / 40019\n",
      "word # 28951 / 40019\n",
      "word # 29001 / 40019\n",
      "word # 29051 / 40019\n",
      "word # 29101 / 40019\n",
      "word # 29151 / 40019\n",
      "word # 29201 / 40019\n",
      "word # 29251 / 40019\n",
      "word # 29301 / 40019\n",
      "word # 29351 / 40019\n",
      "word # 29401 / 40019\n",
      "word # 29451 / 40019\n",
      "word # 29501 / 40019\n",
      "word # 29551 / 40019\n",
      "word # 29601 / 40019\n",
      "word # 29651 / 40019\n",
      "word # 29701 / 40019\n",
      "word # 29751 / 40019\n",
      "word # 29801 / 40019\n",
      "word # 29851 / 40019\n",
      "word # 29901 / 40019\n",
      "word # 29951 / 40019\n",
      "word # 30001 / 40019\n",
      "word # 30051 / 40019\n",
      "word # 30101 / 40019\n",
      "word # 30151 / 40019\n",
      "word # 30201 / 40019\n",
      "word # 30251 / 40019\n",
      "word # 30301 / 40019\n",
      "word # 30351 / 40019\n",
      "word # 30401 / 40019\n",
      "word # 30451 / 40019\n",
      "word # 30501 / 40019\n",
      "word # 30551 / 40019\n",
      "word # 30601 / 40019\n",
      "word # 30651 / 40019\n",
      "word # 30701 / 40019\n",
      "word # 30751 / 40019\n",
      "word # 30801 / 40019\n",
      "word # 30851 / 40019\n",
      "word # 30901 / 40019\n",
      "word # 30951 / 40019\n",
      "word # 31001 / 40019\n",
      "word # 31051 / 40019\n",
      "word # 31101 / 40019\n",
      "word # 31151 / 40019\n",
      "word # 31201 / 40019\n",
      "word # 31251 / 40019\n",
      "word # 31301 / 40019\n",
      "word # 31351 / 40019\n",
      "word # 31401 / 40019\n",
      "word # 31451 / 40019\n",
      "word # 31501 / 40019\n",
      "word # 31551 / 40019\n",
      "word # 31601 / 40019\n",
      "word # 31651 / 40019\n",
      "word # 31701 / 40019\n",
      "word # 31751 / 40019\n",
      "word # 31801 / 40019\n",
      "word # 31851 / 40019\n",
      "word # 31901 / 40019\n",
      "word # 31951 / 40019\n",
      "word # 32001 / 40019\n",
      "word # 32051 / 40019\n",
      "word # 32101 / 40019\n",
      "word # 32151 / 40019\n",
      "word # 32201 / 40019\n",
      "word # 32251 / 40019\n",
      "word # 32301 / 40019\n",
      "word # 32351 / 40019\n",
      "word # 32401 / 40019\n",
      "word # 32451 / 40019\n",
      "word # 32501 / 40019\n",
      "word # 32551 / 40019\n",
      "word # 32601 / 40019\n",
      "word # 32651 / 40019\n",
      "word # 32701 / 40019\n",
      "word # 32751 / 40019\n",
      "word # 32801 / 40019\n",
      "word # 32851 / 40019\n",
      "word # 32901 / 40019\n",
      "word # 32951 / 40019\n",
      "word # 33001 / 40019\n",
      "word # 33051 / 40019\n",
      "word # 33101 / 40019\n",
      "word # 33151 / 40019\n",
      "word # 33201 / 40019\n",
      "word # 33251 / 40019\n",
      "word # 33301 / 40019\n",
      "word # 33351 / 40019\n",
      "word # 33401 / 40019\n",
      "word # 33451 / 40019\n",
      "word # 33501 / 40019\n",
      "word # 33551 / 40019\n",
      "word # 33601 / 40019\n",
      "word # 33651 / 40019\n",
      "word # 33701 / 40019\n",
      "word # 33751 / 40019\n",
      "word # 33801 / 40019\n",
      "word # 33851 / 40019\n",
      "word # 33901 / 40019\n",
      "word # 33951 / 40019\n",
      "word # 34001 / 40019\n",
      "word # 34051 / 40019\n",
      "word # 34101 / 40019\n",
      "word # 34151 / 40019\n",
      "word # 34201 / 40019\n",
      "word # 34251 / 40019\n",
      "word # 34301 / 40019\n",
      "word # 34351 / 40019\n",
      "word # 34401 / 40019\n",
      "word # 34451 / 40019\n",
      "word # 34501 / 40019\n",
      "word # 34551 / 40019\n",
      "word # 34601 / 40019\n",
      "word # 34651 / 40019\n",
      "word # 34701 / 40019\n",
      "word # 34751 / 40019\n",
      "word # 34801 / 40019\n",
      "word # 34851 / 40019\n",
      "word # 34901 / 40019\n",
      "word # 34951 / 40019\n",
      "word # 35001 / 40019\n",
      "word # 35051 / 40019\n",
      "word # 35101 / 40019\n",
      "word # 35151 / 40019\n",
      "word # 35201 / 40019\n",
      "word # 35251 / 40019\n",
      "word # 35301 / 40019\n",
      "word # 35351 / 40019\n",
      "word # 35401 / 40019\n",
      "word # 35451 / 40019\n",
      "word # 35501 / 40019\n",
      "word # 35551 / 40019\n",
      "word # 35601 / 40019\n",
      "word # 35651 / 40019\n",
      "word # 35701 / 40019\n",
      "word # 35751 / 40019\n",
      "word # 35801 / 40019\n",
      "word # 35851 / 40019\n",
      "word # 35901 / 40019\n",
      "word # 35951 / 40019\n",
      "word # 36001 / 40019\n",
      "word # 36051 / 40019\n",
      "word # 36101 / 40019\n",
      "word # 36151 / 40019\n",
      "word # 36201 / 40019\n",
      "word # 36251 / 40019\n",
      "word # 36301 / 40019\n",
      "word # 36351 / 40019\n",
      "word # 36401 / 40019\n",
      "word # 36451 / 40019\n",
      "word # 36501 / 40019\n",
      "word # 36551 / 40019\n",
      "word # 36601 / 40019\n",
      "word # 36651 / 40019\n",
      "word # 36701 / 40019\n",
      "word # 36751 / 40019\n",
      "word # 36801 / 40019\n",
      "word # 36851 / 40019\n",
      "word # 36901 / 40019\n",
      "word # 36951 / 40019\n",
      "word # 37001 / 40019\n",
      "word # 37051 / 40019\n",
      "word # 37101 / 40019\n",
      "word # 37151 / 40019\n",
      "word # 37201 / 40019\n",
      "word # 37251 / 40019\n",
      "word # 37301 / 40019\n",
      "word # 37351 / 40019\n",
      "word # 37401 / 40019\n",
      "word # 37451 / 40019\n",
      "word # 37501 / 40019\n",
      "word # 37551 / 40019\n",
      "word # 37601 / 40019\n",
      "word # 37651 / 40019\n",
      "word # 37701 / 40019\n",
      "word # 37751 / 40019\n",
      "word # 37801 / 40019\n",
      "word # 37851 / 40019\n",
      "word # 37901 / 40019\n",
      "word # 37951 / 40019\n",
      "word # 38001 / 40019\n",
      "word # 38051 / 40019\n",
      "word # 38101 / 40019\n",
      "word # 38151 / 40019\n",
      "word # 38201 / 40019\n",
      "word # 38251 / 40019\n",
      "word # 38301 / 40019\n",
      "word # 38351 / 40019\n",
      "word # 38401 / 40019\n",
      "word # 38451 / 40019\n",
      "word # 38501 / 40019\n",
      "word # 38551 / 40019\n",
      "word # 38601 / 40019\n",
      "word # 38651 / 40019\n",
      "word # 38701 / 40019\n",
      "word # 38751 / 40019\n",
      "word # 38801 / 40019\n",
      "word # 38851 / 40019\n",
      "word # 38901 / 40019\n",
      "word # 38951 / 40019\n",
      "word # 39001 / 40019\n",
      "word # 39051 / 40019\n",
      "word # 39101 / 40019\n",
      "word # 39151 / 40019\n",
      "word # 39201 / 40019\n",
      "word # 39251 / 40019\n",
      "word # 39301 / 40019\n",
      "word # 39351 / 40019\n",
      "word # 39401 / 40019\n",
      "word # 39451 / 40019\n",
      "word # 39501 / 40019\n",
      "word # 39551 / 40019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word # 39601 / 40019\n",
      "word # 39651 / 40019\n",
      "word # 39701 / 40019\n",
      "word # 39751 / 40019\n",
      "word # 39801 / 40019\n",
      "word # 39851 / 40019\n",
      "word # 39901 / 40019\n",
      "word # 39951 / 40019\n",
      "word # 40001 / 40019\n"
     ]
    }
   ],
   "source": [
    "sentiment_list = [] \n",
    "# for word in words.iloc[0:100,0]:\n",
    "for i, word in enumerate (words.iloc[:,0]):\n",
    "    if ( i % 50 == 0): print(\"word #\", i+1, \"/\", len(words))\n",
    "    sentiment_list.append(get_rows_data(word, sentix, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diro</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.307777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gente</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.18750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.475986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>guerra</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.139754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>portato</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.455007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>casa</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.186682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40014</th>\n",
       "      <td>#rip</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40015</th>\n",
       "      <td>#aspettandoprometeo</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40016</th>\n",
       "      <td>#ivreich</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40017</th>\n",
       "      <td>satanasso</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40018</th>\n",
       "      <td>consigliatissima</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40019 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     lemma  positive_score  negative_score  polarity  \\\n",
       "0                     diro          0.1250         0.28125      -1.0   \n",
       "1                    gente          0.4375         0.18750       1.0   \n",
       "2                   guerra          0.0625         0.12500      -1.0   \n",
       "3                  portato          0.1250         0.43750      -1.0   \n",
       "4                     casa          0.0650         0.17500      -1.0   \n",
       "...                    ...             ...             ...       ...   \n",
       "40014                 #rip          0.0000         0.00000       0.0   \n",
       "40015  #aspettandoprometeo          0.0000         0.00000       0.0   \n",
       "40016             #ivreich          0.0000         0.00000       0.0   \n",
       "40017            satanasso          0.0000         0.50000      -1.0   \n",
       "40018     consigliatissima          0.0000         0.00000       0.0   \n",
       "\n",
       "       intensity  \n",
       "0       0.307777  \n",
       "1       0.475986  \n",
       "2       0.139754  \n",
       "3       0.455007  \n",
       "4       0.186682  \n",
       "...          ...  \n",
       "40014   0.000000  \n",
       "40015   0.000000  \n",
       "40016   0.000000  \n",
       "40017   0.500000  \n",
       "40018   0.000000  \n",
       "\n",
       "[40019 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dataframe = pd.DataFrame.from_records(sentiment_list, columns = labels)\n",
    "sentiment_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dataframe.to_csv('lemmas_sentiment.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
